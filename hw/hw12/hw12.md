# Math 4610 Numerical Methods

## Task 1
Apply OpenMP directives to your matrix-vector multiplication routine. Determine if the operation can be improved through OpenMP directives. Test on large matrices to see if the performance is improved. Document this in your software manual.
### Response

### Sources

## Task 2
Use the code from Task 1 to the Jacobi iteration routine to improve performance (if possible). Use the residual vector update version of the code. You should be able to use the matrix-vector code from previous tasks to start this process. Document the work in a page in your software manual.
### Response

### Sources

## Task 3
Include OpenMP in the computation of the largest (in magnitude) eigenvalue of a square matrix, 
A
. Include your work in a page in your software manual.
### Response

### Sources

## Task 4
Repeat Task 3 for the inverse power method for finding the smallest (in magnitude) eigenvalue of a square matrix. Test the work on a square, diagonally-dominant, symmetric matrix and document the work in your software manual. Hint: It will likely work better if you use Jacobi iteration to solve the linear systems.
### Response

### Sources

## Task 5
Combine all of the work in the past two task sheets to compute the 2-norm condition number of a square symmetric matrix. Test the code on a symmetric, diagonally dominant, square matrix of size as large as possible on your computer. There are limitations on most machines. Document your work in your software manual.
### Response

### Sources

## Task 6
Find web sites that discuss the use other platforms like MPI, or OpenACC/CUDA for optimizing code on various types of hardware. Write a brief paragraph (3 or 4 sentences) that describe your findings. Include links to the sites you cite.
### Response
Some portions of code in the GPU cna benefit greatly from parallelization. CUDA-capable GPUs  are designed for maximum parallel throughput. The parallelization of the GPU allows for  increased performances on a GPU around 149 times the original runtime of the code. The only truly open avenue for riding Mooreâ€™s law today is increasing hardware parallelism in several different ways: more computing nodes, more processors in each node, more cores within each processor, and longer vector instructions in each core. This is why parallizing serial programs in hardware applications is becoming a norm today.
### Sources
[Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)
[gpu Programming](https://www.asc.edu/sites/default/files/org_sections/HPC/documents/gpu_cuda_acc_10_small.pdf)
[GPU Tech Conference](https://www.openacc.org/events/gpu-technology-conference)
[Heterogeneous clusters using OpenACC](https://www.hpcwire.com/2017/07/03/optimizing-codes-heterogeneous-hpc-clusters-using-openacc/)
[Multi-GPU Performance Optimization of a CFD Code](https://arxiv.org/pdf/2006.02602.pdf)
[Advanced CUDA and OpenACC](https://materials.prace-ri.eu/377/1/AdvCudaOpenACCLectures.pdf)
[What is the best option for GPU programming](https://www.researchgate.net/post/What-is-the-best-option-for-GPU-programming)
